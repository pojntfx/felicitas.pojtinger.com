---
title: InfraCTL is Ready!
author: Felix Pojtinger
excerpt: A universal CLI for Hetzner Cloud, WireGuard, N2N and k3s
imgSrc: "../assets/articles/infractl-works-header.jpg"
imgAlt: '"In the line of fire" by Christopher Burns on Unsplash'
lastEdit: "2019-09-23"
date: "2019-09-23"
article: true
---

After many, many hours spend on nightly development sessions and daily architecture debate, I am proud to present a first working version of InfraCTL, the universal cloud, VPN and cluster CLI.

> Get [InfraCTl on GitLab](https://gitlab.com/pojntfx/pojntfx/tree/master/packages/infractl)!

Now, what does InfraCTL do exactly?

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl
Usage: infractl [options] [command]

A CLI to manage supra-cloud SSH keys, nodes, WireGuard (OSI Layer 3)/N2N (OSI Layer 2) overlay networks and Kubernetes clusters.

Options:
  -V, --version  output the version number
  -h, --help     output usage information

Commands:
  apply|a        Create or update resources
  get|g          Get resources
  delete|d       Delete resources
  help [cmd]     display help for [cmd]
```

The output above contains a description: As I pointed out before, InfraCTL has the goal of being a universal cloud, VPN and cluster CLI - that means that no client-side tool other than InfraCTL should be necessary to get a working Kubeconfig to a fully-functional Kubernetes cluster.

Let's look at the `apply` commands:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply
Usage: infractl-apply [options] [command]

Options:
  -h, --help                                    output usage information

Commands:
  contexts|context                              Create or update context(s)
  keys|key                                      Create or update key(s)
  nodes|node                                    Create or update node(s)
  privatenetworkclusters|privatenetworkcluster  Create or update private network cluster(s)
  workloadclusters|workloadcluster              Create or update workload cluster(s)
  help [cmd]                                    display help for [cmd]
```

As you can see, InfraCTL is quite similar to `kubectl` - there are simple, declarative models which you can `apply`, `get` and `delete`. Let's go over these models one by one.

As you can probably think, because InfraCTL connects to Hetzner Cloud, at first a context needs to be set up containing the Hetzner Cloud credentials. If you already have some servers set up, feel free to skip the following steps; everything that uses ID prefixes (like `H-`) is bound to a particular provider. Note: This will allow for the use of the Cluster Platform in the future as well!

## Create Nodes

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply context
Usage: infractl-apply-contexts [options]

Options:
  -h, --hetzner-endpoint <endpoint>  Hetzner's endpoint (i.e. https://api.hetzner.cloud/v1)
  -H, --hetzner-token <token>        Hetzner's token (i.e. jEheVytlAoFl7F8MqUQ7jAo2hOXASztX)
  -h, --help                         output usage information
```

Once you've set up the context, you can upload your SSH key:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply key
Usage: infractl-apply-keys [options] <id|"H-">

Options:
  -n, --key-name <name>  Key's name (i.e. user@ip-1)
  -f, --key-file [path]  Key's path (i.e. ~/.ssh/id_rsa.pub) (cannot be updated)
  -h, --help             output usage information
```

Now, let's create a node. But first, we need to take a look at the location, type and OS we want to use! The `infractl get` commands are useful for that.

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get
Usage: infractl-get [options] [command]

Options:
  -h, --help                                              output usage information

Commands:
  keys|key                                                Get key(s)
  locations|location                                      Get location(s)
  types|type                                              Get types(s)
  oses|os                                                 Get OS(es)
  nodes|node                                              Get node(s)
  privatenetworkclustertokens|privatenetworkclustertoken  Get private network cluster token
  privatenetworkclusternodes|privatenetworkclusternode    Get private network cluster nodes(s)
  workloadclustertokens|workloadclustertoken              Get workload cluster token
  workloadclusterconfigs|workloadclusterconfig            Get workload cluster config
  help [cmd]                                              display help for [cmd]
```

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get location
| ID  | NAME | LATITUDE  | LONGITUDE |
| --- | ---- | --------- | --------- |
| H-1 | fsn1 | 50.47612  | 12.370071 |
| H-2 | nbg1 | 49.452102 | 11.076665 |
| H-3 | hel1 | 60.169855 | 24.938379 |
```

You can get more information on a location by specifying it's ID:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get location H-3
id: H-3
name: hel1
latitude: 60.169855
longitude: 24.938379
description: Helsinki DC Park 1, Helsinki, FI

```

Well, let's use the Helsinki datacenter, which has got the ID `H-3`. Next, get a list of all types:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get type
| ID   | NAME  | HOURLY-PRICE | MONTHLY-PRICE |
| ---- | ----- | ------------ | ------------- |
| H-1  | cx11  | 0.0048 €     | 2.9631 €      |
| H-3  | cx21  | 0.0095 €     | 5.831 €       |
| H-5  | cx31  | 0.0167 €     | 10.591 €      |
| H-7  | cx41  | 0.0309 €     | 18.921 €      |
| H-9  | cx51  | 0.0595 €     | 35.581 €      |
| H-11 | ccx11 | 0.0357 €     | 23.681 €      |
| H-12 | ccx21 | 0.0714 €     | 41.531 €      |
| H-13 | ccx31 | 0.1428 €     | 83.181 €      |
| H-14 | ccx41 | 0.2856 €     | 166.481 €     |
| H-15 | ccx51 | 0.5712 €     | 321.181 €     |
```

As you can see, multiple types with varying prices are available. The cheapest one has the following specs:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get type H-1
id: H-1
name: cx11
prices:
  hourly: 0.0048 €
  monthly: 2.9631 €
description: CX11
cores: 1
memory: 2 GB
disk: 20 GB


```

This is perfect for our cluster! We are going to use the `H-1` type. Now to the last parameter: the OS. There are multiple OSes available to choose from:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get os
| ID        | NAME         |
| --------- | ------------ |
| H-1       | ubuntu-16.04 |
| H-2       | debian-9     |
| H-3       | centos-7     |
| H-168855  | ubuntu-18.04 |
| H-5594052 | fedora-30    |
| H-5924233 | debian-10    |
```

InfraCTL currently supports Ubuntu 18.04, Debian 10 and CentOS 7; in case you are using something like a minimal `busybox`-based distro, such a system only works as a compute node and might have networking or storage problems. We are going to use Debian 10 in this case, so the ID `H-5924233` is the one we can use.

Before we can get going, let's take a look at the SSH key we uploaded before and note down it's ID:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get key
| ID        | NAME                                 | FINGERPRINT                                     |
| --------- | ------------------------------------ | ----------------------------------------------- |
| H-1040848 | ubuntu@dev-node-1                    | 92:c8:82:b3:e4:f1:65:9d:aa:c9:8a:53:ef:8a:49:98 |
| H-1040850 | pojntfx@thinkpadx1c3.pojtinger.space | 33:42:a8:d5:d1:69:63:84:56:4b:e0:92:45:d6:5d:e0 |
| H-1045282 | vagrant@dev-node-2.pojtinger.space   | 4b:f0:c9:2d:ae:af:61:36:06:db:33:02:e3:06:63:6c |
```

In my case, that is `H-1040850`.

Now that we've gathered all information, we can continue with the node creation:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply node
Usage: infractl-apply-nodes [options] [id]

Options:
  -n, --node-name <name>    Node's name (i.e. node-1)
  -k, --node-key [id]       Node's key (cannot be updated)
  -l, --node-location [id]  Node's location (cannot be updated)
  -t, --node-type [id]      Node's type (cannot be updated)
  -o, --node-os [id]        Node's OS (cannot be updated)
  -h, --help                output usage information
```

Specify the parameters:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply node -n node-1 -k H-1040850 -l H-3 -t H-1 -o H-5924233
1569333071691 [DATA] Successfully applied node:
id: H-3339515
name: node-1
ips:
  public: 95.216.215.197
location: H-3
type: H-1
os: H-5924233

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

And there we go, you've created your first node! Let's create two more:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply node -n node-2 -k H-1040850 -l H-3 -t H-1 -o H-5924233
# (...)
[pojntfx@thinkpadx1c3 ~]$ infractl apply node -n node-3 -k H-1040850 -l H-3 -t H-1 -o H-5924233
# (...)
[pojntfx@thinkpadx1c3 ~]$ infractl get node
| ID        | NAME   | PUBLIC-IP      | LOCATION | TYPE | OS        |
| --------- | ------ | -------------- | -------- | ---- | --------- |
| H-3339515 | node-1 | 95.216.215.197 | H-3      | H-1  | H-5924233 |
| H-3339517 | node-2 | 95.217.15.29   | H-3      | H-1  | H-5924233 |
| H-3339518 | node-3 | 95.217.15.31   | H-3      | H-1  | H-5924233 |
```

Those should be all nodes for the cluster. Take note of the `PUBLIC-IP` column, which shows the IP addresses of the nodes we'll use in the following. Note: If you already have your own servers and you decided to skip the steps above, continue here!

Before we can access the nodes, we first have to start the SSH agent so that InfraCTL can securely access your SSH private key in memory:

```bash
[pojntfx@thinkpadx1c3 ~]$ eval $(ssh-agent)
Agent pid 27281
```

Now we need to add the SSH key you used above to the agent; you might have to set up a passphrase:

```bash
[pojntfx@thinkpadx1c3 ~]$ ssh-add ~/.ssh/id_rsa
Enter passphrase for /home/pojntfx/.ssh/id_rsa:
Identity added: /home/pojntfx/.ssh/id_rsa (pojntfx@thinkpadx1c3.pojtinger.space)
```

And there you go, you can now login to the nodes with `root@${IP}`!

## Create Private Network Clusters (PNCs)

There are two types of private network clusters, or PNCs for short, available. We are going to deploy both clusters here on all of the nodes; in the process of doing so, you'll see how the `private-ip/public-ip` syntax works and why it exists. I also recommend you do this; it's another layer of security and comfort if you use both PNCs.

### Type 2

So, let's take a look at how to deploy the first PNC:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply privatenetworkcluster
Usage: infractl-apply-privatenetworkclusters [options] <user@manager-node-ip|user@manager-node-public-ip/manager-node-private-ip|manager-node-ip> [user@worker-node-ip|user@worker-node-public-ip/worker-node-private-ip] [otherWorkerNodes...] [$(whoami)@$(hostname)|$(whoami)@$(hostname)/node-private-ip]

Options:
  -t, --private-network-cluster-type [2|3]  Private network clusters' type (OSI layer) (by default 3)
  -k, --token [token]                       Private network cluster's token (if specified, provide just the manager node's ip without the it's user, but specify the worker nodes' users)
  -h, --help                                output usage information
```

As you can see, this action is quite a bit more complicated to use than the former ones. This is due to the `private-ip/public-ip` syntax - while it is a bit more complex, it allows for the installation of one PNC over another PNC as well as an installation that does not require the local node to be modified, which is very useful in our case.

The two types which are a available are `2` and `3`. The types correspond to the [OSI layer 2](https://en.wikipedia.org/wiki/Data_link_layer)/[OSI layer 3](https://en.wikipedia.org/wiki/Network_layer); type 2 is significantly slower than type 3 but supports Ethernet-level traffic, like for example many old LAN-based games require. Using either of the PNCs allows for the cluster to deployed across everything from one or many public clouds to private cloud behind NAT (they can't ping one another directly) using NAT hole punching. The layer 2 PNC is considerably better at the latter use case, while the layer 3 PNC is much, much faster.

So, first we'll deploy the layer 2 PNC manager; you may also simply specify the worker nodes as well, but for demonstration purposes we'll not do this:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply privatenetworkcluster -t 2 root@95.216.215.197
1569335003254 [INFO] Creating provided public network cluster node data model ........ pojntfx@thinkpadx1c3.pojtinger.space
# (...)
1569335034276 [DATA] Successfully applied private network clusters' variables:
managerNode:
  access:
    public: root@95.216.215.197
    private: root@192.168.1.1
token: 5Ll8MrdkpElxAp6ExcFIHHyxnj30w2xYVm0IxBloykQ=

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

Note that you can get the token for the worker nodes later on as well with the following command:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get privatenetworkclustertoken -t 2 root@95.216.215.197
5Ll8MrdkpElxAp6ExcFIHHyxnj30w2xYVm0IxBloykQ=
```

If we take a look at the nodes in the PNC, not much can be found right now:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get privatenetworkclusternodes -t 2 root@95.216.215.197
| ID                                | NAME   | PRIVATE-IP  |
| --------------------------------- | ------ | ----------- |
| not_available_manually_configured | node-1 | 192.168.1.1 |
```

So let's add the other nodes as worker nodes to the PNC, which may take some time:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply privatenetworkcluster -t 2 -k 5Ll8MrdkpElxAp6ExcFIHHyxnj30w2xYVm0IxBloykQ= 95.216.215.197 root@95.217.15.29 root@95.217.15.31
1569335183645 [INFO] Creating provided public network cluster node data model ........ pojntfx@thinkpadx1c3.pojtinger.space
# (...)
1569335229530 [DATA] Successfully applied private network clusters' variables:
managerNode:
  ips:
    public: 95.216.215.197
token: 5Ll8MrdkpElxAp6ExcFIHHyxnj30w2xYVm0IxBloykQ=

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

You may re-run this command at any time to add more nodes to the PNC.

Let's take another look at the nodes in it:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get privatenetworkclusternodes -t 2 root@95.216.215.197
| ID                                | NAME   | PRIVATE-IP   |
| --------------------------------- | ------ | ------------ |
| not_available_manually_configured | node-1 | 192.168.1.1  |
| 02:f8:7f:19:53:f5                 | node-2 | 192.168.1.10 |
| 02:bd:22:72:bb:0e                 | node-3 | 192.168.1.11 |
```

As you can see, the nodes have been added to the PNC. The local node has not been modified; you can't ping the nodes from it:

```bash
[pojntfx@thinkpadx1c3 ~]$ ping -c 1 192.168.1.10
PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
^C
--- 192.168.1.10 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms

```

This is why the `private-ip/public-ip` syntax exists! It will allow us to install the type 3 PNC without modifying the host. In our use case, we will set up the manager and worker nodes in one command; you may however use the step-by-step procedure as shown above as well. A quick note: In the following setup, we let the type 3 worker nodes _connect_ to the type 3 manager node using the type 2 PNC to support a NAT-ed configuration (the nodes can't ping each other without the type 2 PNC), but the _traffic_ between the type 3 nodes themselves _does not go through the type 2 PNC_, which improves the PNC's speed. If you want to route all type 3 PNC traffic over the type 2 PNC, simply specify the private IPs of the worker nodes that you got from above with the `private-ip/public-ip` syntax.

If you want to also install the type 2 PNC on the local node, just add `$(whoami)@$(hostname)` or `$(whoami)@$(hostname)/node-private-ip` to the `apply` command above.

### Type 3

So let's deploy the type 3 PNC:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply privatenetworkcluster -t 3 root@95.216.215.197/192.168.1.1 root@95.217.15.29 root@95.217.15.31
1569335765321 [INFO] Creating provided public network cluster node data model ........ pojntfx@thinkpadx1c3.pojtinger.space
# (...)
1569335879278 [DATA] Successfully applied private network clusters' variables:
managerNode:
  access:
    public: root@95.216.215.197
    private: root@10.224.183.211
token: uCPxiqIfcOuwmWuHfhYMz/EalX2PBSR2jYHwEEEd20E=

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

Now, you may take a look at the type 3 PNC nodes (and token, see the commands above, but don't forget to change `-t 2` to `-t 3`):

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get privatenetworkclusternodes -t 3 root@95.216.215.197
| ID                                           | NAME   | PRIVATE-IP     | PUBLIC-IP      |
| -------------------------------------------- | ------ | -------------- | -------------- |
| not_available_this_is_the_query_node         | node-1 | 10.224.183.211 | 95.216.215.197 |
| mJabtIp6AVRjP2Z9ZDxtACRhIp4PQxueZi+7NNZOjWw= | node-3 | 10.224.186.73  | 95.217.15.31   |
| 0WSF78kJy64do0T0W93eNVb8Mtt96kPlnh6w2HL/2TM= | node-2 | 10.224.185.14  | 95.217.15.29   |
```

That works! And again, because no changes have been made to the local node, you can't ping the nodes on their private IPs. If want to do that, follow the instructions from the type 2 PNC above.

## Create Workload Cluster (WLC)

Now, let's get to deploying the actual workload cluster!

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply workloadcluster
Usage: infractl-apply-workloadclusters [options] <user@manager-node-ip|user@manager-node-public-ip/manager-node-private-ip|manager-node-ip> [user@worker-node-ip] [otherWorkerNodes...] [$(whoami)@$(hostname)]

Options:
  -e, --lets-encrypt-certificate-issuers-email [email]                                        Let's Encrypt certificate issuers' email for certificates (i.e. user@host.tld) (by default the issuers won't be deployed)
  -m, --additional-manager-node-ip [ip]                                                       Additional manager node's IP for the workload cluster config (i.e. 192.168.178.141) (by default the target IP will be used, which might only be reachable from within the private network cluster depending on your setup)
  -t, --private-network-cluster-type [2|3]                                                    Private network clusters' type (OSI layer) (by default 3)
  -k, --token [token]                                                                         Workload cluster's token (if specified, provide just the manager node's ip without the it's user, but specify the worker nodes' users)
  -a, --apply [monitoring,tracing,error-tracking]                                             Comma-seperated list of components which are not applied by default to apply
  -d, --dont-apply [storage,ingress-controller,certificate-manager,virtual-machines,metrics]  Comma-seperated list of components which are applied by default to not apply
  -h, --help                                                                                  output usage information
```

As you can see, there are a lot of options here, and the `private-ip/public-ip` syntax exists as well, though it is only necessary for the manager node, as the rest will be configured automatically.

There are quite some options for the `apply workloadcluster` option; let's go over them one-by-one.

- `-e`: If selected, this sets up [Let's Encrypt](https://letsencrypt.org/) for _automatic_ HTTPS
- `-m`: Here you may specify another IP address than the one you set for the master, which is useful for accessing the cluster outside of the PNCs (we'll use this option)
- `-t`: Here you can set the PNC to use for the workload cluster. We'll use the fast type 3 PNC for this - if you use the type 2 PNC, be sure to use the correct `private-ip` for the manager
- `-a`: Here you can choose which additional components you want to deploy, such as `monitoring` ([Prometheus](https://prometheus.io/)), `tracing` ([Jaeger](https://www.jaegertracing.io/)) and `error-tracking` ([Sentry](https://sentry.io/))
- `-d`: Here you can un-select components installed by default, such as the capability to run [KubeVirt-based](https://kubevirt.io/) virtual machines or the [OpenEBS-based](https://www.openebs.io/) storage cluster.

Now, you can also deploy the manager and workers nodes in one command, but for demonstration purposes we'll deploy them one-by-one; so let's get started with the manager node and add the worker nodes later!

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply workloadcluster root@95.216.215.197/10.224.183.211 -m 95.216.215.197 -t 3
1569336943917 [INFO] Creating provided network cluster node data model ............... pojntfx@thinkpadx1c3.pojtinger.space
# (...)
1569337138926 [DATA] Successfully applied workload clusters' variables:
managerNode:
  access:
    public: root@95.216.215.197
    private: 10.224.183.211
token: K10c1f320e840235610f896259b4448ea2d5b61ac1ea5fd8193e5b4a10141cb3619::node:6060bd88c1901bdf2d73a15e58985205

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
1569337138932 [DATA] Successfully applied workload cluster's config:
apiVersion: v1
# (...)

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

If you want to, you can now copy and paste the contents of the workload cluster config (kubeconfig) file from above into `~/.kube/config` (you might need to create the `~/.kube` directory first), but this is not necessary to join the remaining worker nodes. If you want to to that right now however, your Kubernetes nodes should look like this (from `kubectl get nodes`):

```bash
[pojntfx@thinkpadx1c3 ~]$ kubectl get node
NAME     STATUS   ROLES    AGE    VERSION
node-1   Ready    master   119s   v1.15.3-k3s.1
```

If you don't plan on adding any more nodes to the workload cluster, feel free to skip the following section.

In order to join the worker nodes into the workload cluster, you can either copy and paste the token from above, or get it at any time with the following command:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get workloadclustertoken root@95.216.215.197
K10c1f320e840235610f896259b4448ea2d5b61ac1ea5fd8193e5b4a10141cb3619::node:6060bd88c1901bdf2d73a15e58985205
```

Now, join the rest of the nodes as worker nodes into the cluster (depending on the PNC you chose the manager's IP might be different):

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl apply workloadcluster -t 3 -k K10c1f320e840235610f896259b4448ea2d5b61ac1ea5fd8193e5b4a10141cb3619::node:6060bd88c1901bdf2d73a15e58985205 10.224.183.211 root@95.217.15.29 root@95.217.15.31
1569337601368 [INFO] Creating provided network cluster node data model ............... pojntfx@thinkpadx1c3.pojtinger.space
# (...)
1569337797818 [DATA] Successfully applied workload clusters' variables:
managerNode:
  ips:
    public: 10.224.183.211
token: K10c1f320e840235610f896259b4448ea2d5b61ac1ea5fd8193e5b4a10141cb3619::node:6060bd88c1901bdf2d73a15e58985205

...................................................................................... pojntfx@thinkpadx1c3.pojtinger.space
```

And there we go! Now you can copy and paste the workload cluster config (kubeconfig) as described above or get it again with the following command:

```bash
[pojntfx@thinkpadx1c3 ~]$ infractl get workloadclusterconfig root@95.216.215.197
apiVersion: v1
# (...)
```

Once you've set this up, check if the worker nodes have been added successfully:

```bash
[pojntfx@thinkpadx1c3 ~]$ kubectl get node -o wide
NAME     STATUS   ROLES    AGE    VERSION         INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                       KERNEL-VERSION   CONTAINER-RUNTIME
node-1   Ready    master   12m    v1.15.3-k3s.1   10.224.183.211   <none>        Debian GNU/Linux 10 (buster)   4.19.0-6-amd64   containerd://1.2.8-k3s.1
node-3   Ready    worker   108s   v1.15.3-k3s.1   10.224.186.73    <none>        Debian GNU/Linux 10 (buster)   4.19.0-6-amd64   containerd://1.2.8-k3s.1
node-2   Ready    worker   107s   v1.15.3-k3s.1   10.224.185.14    <none>        Debian GNU/Linux 10 (buster)   4.19.0-6-amd64   containerd://1.2.8-k3s.1
```

Take a look the `INTERNAL-IP` column; the type 3 PNC is being used successfully!

Before you can get started, you might have to wait a bit until all the pods for the sub-systems (storage cluster etc.) have started, which can take some time depending on the nodes' internet connection:

```bash
[pojntfx@thinkpadx1c3 ~]$ kubectl get pod -A
NAMESPACE     NAME                                             READY   STATUS      RESTARTS   AGE
kube-system   coredns-55c55bb5db-8mtgw                         1/1     Running     0          14m
kubevirt      virt-handler-49kvm                               1/1     Running     0          14m
kubevirt      virt-controller-564dc766f5-jkmh5                 1/1     Running     0          14m
kubevirt      virt-api-867d55bf86-9thwj                        1/1     Running     0          14m
kubevirt      virt-controller-564dc766f5-972l5                 1/1     Running     0          14m
kubevirt      virt-api-867d55bf86-ms6pg                        1/1     Running     0          14m
kube-system   helm-install-metrics-server-7cjws                0/1     Completed   0          14m
kube-system   helm-install-nginx-ingress-fj6t9                 0/1     Completed   0          14m
default       nginx-ingress-default-backend-576b86996d-pp5jt   1/1     Running     0          12m
kube-system   helm-install-openebs-gp6pr                       0/1     Completed   0          14m
kube-system   helm-install-cert-manager-6pgjc                  0/1     Completed   0          14m
default       metrics-server-77f4b87964-7l49g                  1/1     Running     0          12m
default       cert-manager-cainjector-5d448d76df-cd84w         1/1     Running     0          12m
default       openebs-localpv-provisioner-75d89ff558-4hpsc     1/1     Running     0          12m
default       cert-manager-b74cfcd9d-drqz4                     1/1     Running     0          12m
default       openebs-snapshot-operator-6f4d4c5656-27sqr       2/2     Running     0          12m
default       openebs-ndm-7lgff                                1/1     Running     0          12m
default       openebs-provisioner-65fffd59cc-qgkrr             1/1     Running     0          12m
default       openebs-admission-server-7b567f9c8c-bfhd4        1/1     Running     0          12m
default       openebs-apiserver-5d66955bc5-wfzh8               1/1     Running     2          12m
default       openebs-ndm-operator-5bfb5ff47-xk5mt             1/1     Running     1          12m
default       cert-manager-webhook-6f75cfcbc8-gxmx6            1/1     Running     0          12m
default       nginx-ingress-controller-2sh87                   1/1     Running     0          12m
kubevirt      virt-handler-mwfzn                               1/1     Running     0          3m40s
default       openebs-ndm-s46nk                                1/1     Running     1          3m40s
default       nginx-ingress-controller-d9xj9                   1/1     Running     0          3m39s
default       nginx-ingress-controller-7tx5b                   1/1     Running     0          3m40s
kubevirt      virt-handler-bzhmv                               1/1     Running     1          3m39s
default       openebs-ndm-bnvkh                                1/1     Running     1          3m39s
```

And there you go! You can now join more nodes into the PNCs and/or the WLCs with the commands shown above - or just enjoy your Kubernetes cluster!

And in case you find any problems, feel free to add an issue and/or contribute: [InfraCTl is free/libre and open source software on GitLab](https://gitlab.com/pojntfx/pojntfx/tree/master/packages/infractl)!
